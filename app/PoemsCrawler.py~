import csv, jieba, time, os
from gensim import corpora, similarities, models
import numpy as np

def loadData():
    fr = open('poetry_dataset.csv', 'r')
    result = []
    for line in fr.readlines()[1:]:
        result.append(line.strip(). split('\t'))
    fr.close()
    return result

def loadStopwrods():
    fr = open('Chinese_stopwords.txt', 'r')
    stop_words = []
    for line in fr.readlines():
        stop_words.append(line.strip())
    return stop_words

def parseWords(poems):
    # the format pf poems is : [ '***|***', '&&&|&&&', '%%%|%%%' ]
    # the format of return value is : [ ['*', '*', '*'], ['&', '&', '&'], ['%','%', '%'] ]
    res = []; stop_words = loadStopwrods()
    print('There are ', len(stop_words), 'stop words')
    for poem in poems: # the format of poem is : '***|***'
        words_of_poem = []
        for sentence in poem.split('|'): # the format of sentence is :***
           seg_list = list(jieba.cut(sentence, cut_all=True)) # the format of seg_list is : ['*', '*', '*'] 
           # cut_all = True, leave out all of notations, including '|'
           seg_list = [word for word in seg_list if word not in stop_words]
           #print(seg_list)
           words_of_poem.extend(seg_list)
        res.append(words_of_poem)
    return res

def id2word(word2id):
    i2w = {}
    for key in word2id.keys():
        i2w[word2id[key]] = key
    return i2w

def get_topics_num(topics, num_of_topics=5):
    counts = np.zeros(num_of_topics)
    for topic in topics:
        for ti, _ in topic:
            counts[ti] += 1
        return counts

def get_feature_words(model, topics_num, condition='max'):
    all_top = []
    for i in range(100):
        words = model.show_topic(i)
        all_top.append(sum([ele[1] for ele in words]))
    print(all_top)
    if condition == 'max':
        words = model.show_topic(topics_num.argmax(), 100)
        print(sum([ele[1] for ele in words]))
        return words
    elif condition == 'min':
        words = model.show_topic(topics_num.argmin(), 100)
        print(sum([ele[1] for ele in words]))
        return words
    else:
        return -1

def get_word_freq(words_occur):
    # the format of words_occur is : [[(index_of_word : num_of_occurrence), ()], [ (), () ]]
    words_count = {}
    for t in words_occur:
        for ele in t:
            words_count[ele[0]] = words_count.get(ele[0], 0) + ele[1]
    return words_count # words_count's format should be {index_of_word : freq_of_word}

def count_freq_equal_n_words(words_count):
    # I wanna count how many words occur in 1, 2, 3, and more times
    stat = [0, 0, 0, 0]
    for key in words_count.keys():
        if words_count[key] == 1:
            stat[0] += 1
        elif words_count[key] == 2:
            stat[1] += 1
        elif words_count[key] == 3:
            stat[2] += 1
        else:
            stat[3] += 1
    return stat

def get_corpus_and_dict(raw_set, corpus_name, dict_name):
    # Give me two path names of corpus and dict
    all_poems = [ele[3] for ele in raw_set[:2200]] # this can't work, due to missed value
    if os.path.exists(corpus_name) and os.path.exists(dict_name): # if both of files are existed, load them
        print("mm file and dict file are existed")
        words_ids = corpora.Dictionary.load(dict_name)
        poems_corpus = corpora.MmCorpus(corpus_name)
    else: # If not, create them and save them for use of next time
        print("Craeting new corpus and dictionary")
        parsedwords = parseWords(all_poems)
        words_ids = corpora.Dictionary(parsedwords)
        poems_corpus = [words_ids.doc2bow(ele) for ele in parsedwords]
        words_ids.save(corpus_name)
        corpora.MmCorpus.serialize(corpus_name, poems_corpus)
    return poems_corpus, words_ids

def main():
    dataset = loadData()
    poems_corpus, words_ids = get_corpus_and_dict(dataset, './parsedWords.mm', './parsedWords.dict')
    words_count = get_word_freq(poems_corpus)
    stat_freq = count_freq_equal_n_words(words_count)
    print("There are %d words occurred once; %d twice; %d 3 times; %d over 3 times" \
            % (stat_freq[0], stat_freq[1], stat_freq[2], stat_freq[3]))
    print(sum(stat_freq))
    print("Thee are totally %d words in our corpus" % (len(words_ids)))
    #print(sampleCorpus)
    # It's high time to fit LDA model
    num_topics = 100
    model = models.ldamodel.LdaModel(poems_corpus, num_topics = num_topics, id2word = id2word(words_ids.token2id))
    poems_topics = []
    for poem in poems_corpus:
        poems_topics.append(model[poem])
    counts = get_topics_num(poems_topics, num_of_topics = num_topics)
    print(get_feature_words(model, counts, 'max'))
    print(get_feature_words(model, counts, 'min'))

if __name__=='__main__':
    start = time.time()
    main()
    print("It takes you : (s) ", time.time()-start)
'''
def loadCSV():
	result = []
	reader = csv.reader(open('poetry_dataset.csv', 'r'), dialect='tsv')
	for poem in reader:
		result.append(poem)
	return result 
def loadMatrix():
    data_type = {'names':('id', 'author', 'title', 'text'),
                 'formats':('i4', 'S5', 'S30', 'S1500')}
    matrix = np.loadtxt('poetry_dataset.csv', delimiter = '\t', dtype=data_type, skiprows=1)
    return matrix
'''

